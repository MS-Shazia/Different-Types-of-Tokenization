{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f07fc290",
   "metadata": {},
   "source": [
    "# Types of Tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96540ad",
   "metadata": {},
   "source": [
    "There are different types of tokenization\n",
    "1. Word Tokenization\n",
    "2. Sentence Tokenization\n",
    "3. White Space Tokenization\n",
    "4. Punctuation_based Tokenization\n",
    "5. Treebank Tokenization\n",
    "6. Tweet Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0d5ad9",
   "metadata": {},
   "source": [
    "# 1. Word Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8cf3403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'sole', 'meaning', 'of', 'life', 'is', 'to', 'serving', 'humanity']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "words_tokenize=\"The sole meaning of life is to serving humanity\"\n",
    "print(word_tokenize(words_tokenize))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3859cc5",
   "metadata": {},
   "source": [
    "# 2. Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36a4855f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Working diligently is as important as making an honest living.', 'I believe in many things that i cannot see with my eyes.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sentence_tokenize=\"Working diligently is as important as making an honest living. I believe in many things that i cannot see with my eyes.\"\n",
    "print(sent_tokenize(sentence_tokenize))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f4d98c",
   "metadata": {},
   "source": [
    "# 3. White Space Tokenization in words and sentence using .split() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66fbc0a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Working',\n",
       " 'diligently',\n",
       " 'is',\n",
       " 'as',\n",
       " 'important',\n",
       " 'as',\n",
       " 'making',\n",
       " 'an',\n",
       " 'honest',\n",
       " 'living.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "white_space=\"Working diligently is as important as making an honest living.\"\n",
    "white_space.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f81920dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Working diligently is as important as making an honest living',\n",
       " ' I believe in many things that i cannot see with my eyes',\n",
       " '']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "white_space_sent=\"Working diligently is as important as making an honest living. I believe in many things that i cannot see with my eyes.\"\n",
    "white_space_sent.split(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7d8415",
   "metadata": {},
   "source": [
    "# 4. Punctuation_based Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50cdfd6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', '‚Äô', 've', 'been', 'working', 'hard', 'all', 'day', ',‚Äù', 'he', 'said', '.', '‚Äú', 'Let', '‚Äô', 's', 'go', 'out', 'to', 'dinner', '!.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "wordpunct=\"I‚Äôve been working hard all day,‚Äù he said. ‚ÄúLet‚Äôs go out to dinner!.\"\n",
    "print(wordpunct_tokenize(wordpunct))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25644d1c",
   "metadata": {},
   "source": [
    "# 5.Treebank Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ef7bc630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'you', 'do', \"n't\", 'want', 'to', 'be', 'done', 'to', 'be', 'done', 'to', 'yourself', 'do', \"n't\", 'do', 'the', 'others', '...']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tbw_tokenize=TreebankWordTokenizer()\n",
    "treebank_word=\"What you don't want to be done to be done to yourself don't do the others...\"\n",
    "\n",
    "print(tbw_tokenize.tokenize(treebank_word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d379221",
   "metadata": {},
   "source": [
    "# 6. Tweet Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e6684460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'you', \"don't\", 'want', 'to', 'be', 'done', 'to', 'be', 'done', 'to', 'yourself', \"don't\", 'do', 'the', 'others', '...', 'üòä', 'üòç', 'üòÅ', 'üòí']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "tweet=TweetTokenizer()\n",
    "\n",
    "Tweet_tokenize=\"What you don't want to be done to be done to yourself don't do the others...üòäüòçüòÅüòí\"\n",
    "print(tweet.tokenize(Tweet_tokenize))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
